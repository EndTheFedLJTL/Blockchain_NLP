{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1d272c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(k)? (env.py, line 51)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3369\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m in \u001b[0;35m<cell line: 15>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from env import github_token, github_username\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/env.py:51\u001b[0;36m\u001b[0m\n\u001b[0;31m    print k\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(k)?\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "#import repo_github_api_acquire as aq\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "\n",
    "from env import github_token, github_username\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.github_api_request(url: str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ccdb8f",
   "metadata": {},
   "source": [
    "# Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762fc763",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_token = \"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\"\n",
    "github_username = \"LuisVArce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96273202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_urls(num=5000):\n",
    "    ''' this function scrapes the cryptography repositories from github and returns a list of urls\n",
    "    '''\n",
    "    num_of_repos=num\n",
    "\n",
    "    page_numbers = [i for i in range(0,101)]\n",
    "    print(page_numbers)\n",
    "    urls = [f'https://github.com/search?p={i}&q=%23defi&type=Repositories&per_page=100' for i in page_numbers]\n",
    "\n",
    "    print(urls)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f747816",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_urls()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_endpoints(url):\n",
    "    ''' This function gets the endpoints from the list of above urls\n",
    "    '''\n",
    "\n",
    "    headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "    \n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.ok:\n",
    "            break\n",
    "        else:\n",
    "            print('sleeping')\n",
    "            time.sleep(20)\n",
    "            continue\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    print(response.ok)\n",
    "\n",
    "    endpoints = []\n",
    "    subgroups = soup.find_all('div', {\"class\":\"f4 text-normal\"})\n",
    "\n",
    "    for group in subgroups:\n",
    "        endpoints.append(re.search('href=\".*\"', str(group))[0][6:-1])\n",
    "\n",
    "    return endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b598a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoints('https://github.com/search?p=100&q=%23defi&type=Repositories&per_page=100')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9767474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_all_endpoints():\n",
    "    ''' This function returns all of the endpoints\n",
    "    '''\n",
    "    urls = create_urls()\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "    all_endpoints = []\n",
    "\n",
    "    for i, page in enumerate(urls):\n",
    "        all_endpoints.append(get_endpoints(page))\n",
    "        print(page)\n",
    "\n",
    "    print(len(all_endpoints))\n",
    "\n",
    "    return all_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d85749",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_all_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_endpoints():\n",
    "    ''' This function acquires all endpoints and writes them to a csv.\n",
    "    '''\n",
    "    our_endpoints = pd.Series(make_all_endpoints(), name='endpoints')\n",
    "    our_endpoints.to_csv('endpoints.csv', index=False)\n",
    "\n",
    "    return our_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=acquire_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f80d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105161a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_endpoints():\n",
    "    ''' This function flattens a 2d array into a 1d array\n",
    "    '''\n",
    "    end_points = pd.read_csv('endpoints.csv')\n",
    "    all_values = []\n",
    "    for value in end_points.values:\n",
    "        for ep in value:\n",
    "            all_values.append(ep)\n",
    "\n",
    "    final_values = []\n",
    "    #print(all_values)\n",
    "    for value in all_values:\n",
    "        for val in value.split(\"'\"):\n",
    "            if len(val) > 3:\n",
    "                final_values.append(val)\n",
    "                print(val)\n",
    "\n",
    "    return pd.Series(final_values, name='endpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_values = flatten_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a161c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos=final_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPOS = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524deb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "    raise Exception(\n",
    "        \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5194181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34272c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_api_request(url: str) -> Union[List, Dict]:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Error response from github api! status code: {response.status_code}, \"\n",
    "            f\"response: {json.dumps(response_data)}\"\n",
    "        )\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62da71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_language(repo: str) -> str:\n",
    "    url = f\"https://api.github.com/repos/{repo}\"\n",
    "    repo_info = github_api_request(url)\n",
    "    if type(repo_info) is dict:\n",
    "        repo_info = cast(Dict, repo_info)\n",
    "        if \"language\" not in repo_info:\n",
    "            raise Exception(\n",
    "                \"'language' key not round in response\\n{}\".format(json.dumps(repo_info))\n",
    "            )\n",
    "        return repo_info[\"language\"]\n",
    "    raise Exception(\n",
    "        f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "    contents = github_api_request(url)\n",
    "    if type(contents) is list:\n",
    "        contents = cast(List, contents)\n",
    "        return contents\n",
    "    raise Exception(\n",
    "        f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f044da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Takes in a response from the github api that lists the files in a repo and\n",
    "    returns the url that can be used to download the repo's README file.\n",
    "    \"\"\"\n",
    "    for file in files:\n",
    "        if file[\"name\"].lower().startswith(\"readme\"):\n",
    "            return file[\"download_url\"]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881560ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_repo(repo: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "    dictionary with the language of the repo and the readme contents.\n",
    "    \"\"\"\n",
    "    contents = get_repo_contents(repo)\n",
    "    readme_download_url = get_readme_download_url(contents)\n",
    "    if readme_download_url == \"\":\n",
    "        readme_contents = \"\"\n",
    "    else:\n",
    "        readme_contents = requests.get(readme_download_url).text\n",
    "    return {\n",
    "        \"repo\": repo,\n",
    "        \"language\": get_repo_language(repo),\n",
    "        \"readme_contents\": readme_contents,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17042bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_github_data() -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Loop through all of the repos and process them. Returns the processed data.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for repo in REPOS:\n",
    "        url = f'https://api.github.com/repos{repo}/contents/'\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            print(f'Skipping {repo} because its HTTP status code is {response.status_code}')\n",
    "            continue\n",
    "        contents = response.json()\n",
    "        readme_download_url = get_readme_download_url(contents)\n",
    "        if readme_download_url == \"\":\n",
    "            readme_contents = \"\"\n",
    "        else:\n",
    "            response = requests.get(readme_download_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f'Skipping {repo} because its HTTP status code is {response.status_code}')\n",
    "                continue\n",
    "            readme_contents = requests.get(readme_download_url).text\n",
    "        print(repo)\n",
    "        result = {\n",
    "            \"repo\": repo,\n",
    "            \"language\": get_repo_language(repo),\n",
    "            \"readme_contents\": readme_contents,\n",
    "        }\n",
    "        output.append(result)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad2178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12735584",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ''' This says howdy if everything is acquired correctly. \n",
    "    '''\n",
    "    df = acquire_endpoints()\n",
    "    data = scrape_github_data()\n",
    "    json.dump(data, open(\"data.json\", \"w\"), indent=1)\n",
    "    print('howdy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610d492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff484de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb47c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk: natural language toolkit -> tokenization, stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    ''' Receives a string of text, processes it & then returns its normalized version.\n",
    "    Normalization via standard NKFD unicode, fed into an ASII encoder & decoded back into UTF-8.\n",
    "    '''\n",
    "    string = string.lower()\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", ' ', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ad4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    ''' This function takes in a string and returns the tokenized form\n",
    "    '''\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    string = (tokenizer.tokenize(string, return_str=True))\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''This function takes in a string and returns the stemmed form\n",
    "    '''\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43840fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    ''' This function takes in a string and returns the lemmatized form\n",
    "    '''\n",
    "\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    string = ' '.join(lemmas)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e523450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    ''' This function takes in a string and removes stop words from it\n",
    "    '''\n",
    "    additional_stopwords = ['github', 'http', 'code']\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "    stopword_list = stopwords.words('english') + additional_stopwords\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    words = string.split()\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf9c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_github_data(df):\n",
    "    '''\n",
    "    Takes in a cleaned github dataframe, splits it into train, validate and test subgroups and then returns those subgroups.\n",
    "    Arguments: df - a cleaned pandas dataframe with the expected feature names and columns in the github dataset\n",
    "    Return: train, validate, test - dataframes ready for the exploration and model phases.\n",
    "    '''\n",
    "\n",
    "    train_validate, test = train_test_split(df, test_size=.2, \n",
    "        random_state=17)\n",
    "\n",
    "    train, validate = train_test_split(train_validate, test_size=.3, \n",
    "        random_state=17)\n",
    "\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcc2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_csv():\n",
    "    ''' This function searches for a file named final_data.csv. If found, it returns the data. If not, it will \n",
    "    open data.json, prepare it, create a file named final_data.csv, and then open the file. \n",
    "    '''\n",
    "    filename = 'final_data.csv'\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "\n",
    "    else:\n",
    "        file = open('data.json')\n",
    "\n",
    "        data = json.load(file)\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "        data = data.assign(cleaned=data.readme_contents.apply(basic_clean))\n",
    "        data = data.assign(without_stop_words=data.cleaned.apply(remove_stopwords))\n",
    "        data = data.assign(tokenized=data.without_stop_words.apply(tokenize))\\\n",
    "                .assign(cleaned= data.without_stop_words.apply(remove_stopwords))\\\n",
    "                .assign(stem=data.without_stop_words.apply(stem))\\\n",
    "                .assign(lemm=data.without_stop_words.apply(lemmatize))\n",
    "\n",
    "        excluded_languages = list(data.language.value_counts()[data.language.value_counts() < 6].index)\n",
    "        data =data[~data.language.isin(excluded_languages)]\n",
    "\n",
    "        data.to_csv(filename, index=False)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf017d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ''' This function opens data from the create final csv function\n",
    "    '''\n",
    "    data = create_final_csv()\n",
    "    print(data)\n",
    "    print(data.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6a0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
