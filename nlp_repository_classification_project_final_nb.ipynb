{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fcabc7",
   "metadata": {},
   "source": [
    "### Luis Arce, Lindy Castellaw, Jeremy Lagunas, Tim Keriazes\n",
    "\n",
    "#### Sep 8, 2022\n",
    "\n",
    "# NLP Github Classification Project - EndTheFed - Predicting #defi Repository Program Language From Readme Contents\n",
    "\n",
    "#### Scenario\n",
    "\n",
    "#### Project Description:\n",
    "##### Utilizing the contents of readme files within repositories on github, we set out to build a predictive model that would allow us to determine the programming language of the repository/project based on the contents of the readme. Utilizing web scraping techniques, we built a dataset containing the name of the repository, the programming language utilized, and the contents of the readme. Through some basic cleaning and preparation we will explore the dataset, looking for patterns, and creating some visualizations that show the most common words in the readme, whether the length varies by programming language, unique words, and any other identifying features. We will utilize the TfidfVectorizer() to transform the contents of the readme into features that will be fed into our predictive classification models.\n",
    "\n",
    "\n",
    "Project Planning/Outline:\n",
    "1. Investigating the repository search #defi\n",
    "2. Capture the urls/endpoints for all repos\n",
    "3. Flatten endpoints/utilize acquire.py file to interact with github api to pull repo name, language, and readme contents\n",
    "4. Data preparation/utilize prepare.py file to clean and prep data \n",
    "5. Exploration and Data Analysis/visualize dataset with analysis, word clouds, bigrams\n",
    "6. Split the dataset\n",
    "7. Model utilizing Tfidf vectorized readme contents\n",
    "8. Evaluate Models\n",
    "9. Test\n",
    "10. Conclusions/Next Steps\n",
    " \n",
    "\n",
    "#### Hypothesis\n",
    "1. Contents of readme will be indicative of programming language used\n",
    "\n",
    "### Target variable\n",
    "#### language\n",
    "\n",
    "### Exploration Key Findings/Results\n",
    "Explore and visualize the natural language data that you have acquired. Here are some ideas for exploration:\n",
    "\n",
    "#### 1. The most 10 most common words and the number of appearances in the readme were:\n",
    "        \n",
    "        contract    3914\n",
    "        token       3460\n",
    "        run         1740\n",
    "        1           1722\n",
    "        smart       1622\n",
    "        ethereum    1530\n",
    "        project     1530\n",
    "        address     1503\n",
    "        detail      1499\n",
    "        install     1438\n",
    "        \n",
    "\n",
    "#### 2. Length of the readme did vary by programming language\n",
    "#### 3. Javascript and Typescript utilized numeric characters/words much more so than other programming languages.\n",
    "#### 4. Of the 1000 repositories that we pulled, the most common language breakdown is as follows:\n",
    "\n",
    "        JavaScript\t\t0.237624\n",
    "        TypeScript\t\t0.221782\n",
    "        Solidity\t\t0.165347\n",
    "        Not Specified\t0.127723\n",
    "        Python\t\t    0.058416\n",
    "        Rust\t\t    0.031683\n",
    "        Go\t\t        0.025743\n",
    "\n",
    "#### 5. The top ten bigrams that appeared and their number of occurences across all languages were:\n",
    "\n",
    "        (smart, contract)                           1348\n",
    "        (git, clone)                                 759\n",
    "        (detail, detailssummaryba)                   670\n",
    "        (styledisplayinline, width13)                414\n",
    "        (npm, install)                               350\n",
    "        (srchttpsgitioj9co9, styledisplayinline)     342\n",
    "        (codeimg, srchttpsgitioj9co9)                341\n",
    "        (npm, run)                                   321\n",
    "        (decimal, 18)                                315\n",
    "        (chainid, 1)                                 309\n",
    "\n",
    "\n",
    "# Best Model: \n",
    "### Refined Random Forest with max depth 6\n",
    "    - We established baseline using the mode language of Javascript which appeared 0.27 of all occurrences. Our final model predictions on the test set are 0.71\n",
    "\n",
    "### Key takeaways\n",
    "    - Javascript was the most occurring language \n",
    "    - All models beat the baseline \n",
    "### Next Steps\n",
    "    - Bring in other features like bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ef0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "#import repo_github_api_acquire as aq\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import nlp_acquire\n",
    "import nlp_prepare\n",
    "import prepare_repos\n",
    "\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "from env import github_token, github_username\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02879909",
   "metadata": {},
   "source": [
    "### CreateURLs, get URL endpoints, flatten endpoints, establish final values, write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates Urls\n",
    "nlp_acquire.create_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes endpoints\n",
    "nlp_acquire.make_all_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acquire endpoints and write csv to local\n",
    "nlp_acquire.acquire_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6356c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv\n",
    "final_values = pd.read_csv('endpoints.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6400f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten values of df, Put to list so it can be used in helper file repo_github_api_acquire \n",
    "final_values = nlp_acquire.flatten_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish variable of final values for helper function\n",
    "REPOS = final_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789a7ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilize helper repo_github_api_acquire , input final_values.tolist() produces data.json on local machine\n",
    "#allows the json file to be read in as data\n",
    "with open('data.json', 'r') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d2a175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/OffcierCia/DeFi-Developer-Road-Map</td>\n",
       "      <td>None</td>\n",
       "      <td># DeFi Developer Road Map\\n\\n**Here we collect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/smartcontractkit/full-blockchain-solidity-cou...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;!-- [YouTube Video](https://www.youtube.com/w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/rainbow-me/rainbow</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>![](https://pbs.twimg.com/profile_banners/1103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Bytom/bytom</td>\n",
       "      <td>Go</td>\n",
       "      <td>Bytom\\n======\\n\\n[![Build Status](https://trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/DimensionDev/Maskbook</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>&lt;!-- cspell:disable --&gt;\\n&lt;!-- markdownlint-dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                repo    language  \\\n",
       "0                /OffcierCia/DeFi-Developer-Road-Map        None   \n",
       "1  /smartcontractkit/full-blockchain-solidity-cou...        None   \n",
       "2                                /rainbow-me/rainbow  TypeScript   \n",
       "3                                       /Bytom/bytom          Go   \n",
       "4                             /DimensionDev/Maskbook  TypeScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  # DeFi Developer Road Map\\n\\n**Here we collect...  \n",
       "1  <!-- [YouTube Video](https://www.youtube.com/w...  \n",
       "2  ![](https://pbs.twimg.com/profile_banners/1103...  \n",
       "3  Bytom\\n======\\n\\n[![Build Status](https://trav...  \n",
       "4  <!-- cspell:disable -->\\n<!-- markdownlint-dis...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read it into a df\n",
    "df = pd.read_json('data.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a6c99",
   "metadata": {},
   "source": [
    "### Repos scraped, language and contents pulled, readme contents ready to be cleaned and prepped\n",
    "- Lowercase everything\n",
    "- Normalize unicode characters\n",
    "- Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "- handle nulls\n",
    "- tokenize\n",
    "- remove stopwords\n",
    "- lemmatize\n",
    "- create new df with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ce3d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repo                 0\n",
       "language           129\n",
       "readme_contents      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49c7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle the nulls\n",
    "df[\"language\"].fillna(\"Not Specified\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42326b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_languages = df.language.value_counts()[df.language.value_counts() > 4].index\n",
    "# Removing languages not kept\n",
    "df = df[df.language.isin(remaining_languages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f54f94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df.readme_contents.apply(prepare_repos.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ecf4f21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/OffcierCia/DeFi-Developer-Road-Map</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>defi developer road map collect discus best de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/smartcontractkit/full-blockchain-solidity-cou...</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>youtube video http www youtube com watch v m57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/rainbow-me/rainbow</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>http pb twimg com profile banner 1103191459409...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Bytom/bytom</td>\n",
       "      <td>Go</td>\n",
       "      <td>bytom build status http travis ci org bytom by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/DimensionDev/Maskbook</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>cspell disable markdownlint disable inline htm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                repo       language  \\\n",
       "0                /OffcierCia/DeFi-Developer-Road-Map  Not Specified   \n",
       "1  /smartcontractkit/full-blockchain-solidity-cou...  Not Specified   \n",
       "2                                /rainbow-me/rainbow     TypeScript   \n",
       "3                                       /Bytom/bytom             Go   \n",
       "4                             /DimensionDev/Maskbook     TypeScript   \n",
       "\n",
       "                                     readme_contents  \n",
       "0  defi developer road map collect discus best de...  \n",
       "1  youtube video http www youtube com watch v m57...  \n",
       "2  http pb twimg com profile banner 1103191459409...  \n",
       "3  bytom build status http travis ci org bytom by...  \n",
       "4  cspell disable markdownlint disable inline htm...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f15f9a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b032b",
   "metadata": {},
   "source": [
    "### Exploration, Data Analysis, Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d69af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#percentage of languages\n",
    "\n",
    "labels = pd.concat([df.language.value_counts(),\n",
    "                    df.language.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bcf23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9e8a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_repo_content = nlp_acquire.read_url_or_file_repo()\n",
    "repo_df = nlp_prepare.make_dataframe(scrape_repo_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ea8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_df = nlp_prepare.make_dataframe(scrape_repo_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec37fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_STOPWORDS = []\n",
    "\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking out the words of each of the most used languages\n",
    "js_words = clean(' '.join(repo_df[repo_df.language == 'JavaScript'].readme_contents))\n",
    "ts_words = clean(' '.join(repo_df[repo_df.language == 'TypeScript'].readme_contents))\n",
    "notspec_words = clean(' '.join(repo_df[repo_df.language == 'Not Specified'].readme_contents))\n",
    "solidity_words = clean(' '.join(repo_df[repo_df.language == 'Solidity'].readme_contents))\n",
    "python_words= clean(' '.join(repo_df[repo_df.language == 'Python'].readme_contents))\n",
    "rust_words= clean(' '.join(repo_df[repo_df.language == 'Rust'].readme_contents))\n",
    "go_words= clean(' '.join(repo_df[repo_df.language == 'Go'].readme_contents))\n",
    "\n",
    "all_words = clean(' '.join(repo_df.readme_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of the words\n",
    "js_freq = pd.Series(js_words).value_counts()\n",
    "ts_freq = pd.Series(ts_words).value_counts()\n",
    "notspec_freq = pd.Series(notspec_words).value_counts()\n",
    "solidity_freq = pd.Series(solidity_words).value_counts()\n",
    "python_freq = pd.Series(python_words).value_counts()\n",
    "rust_freq = pd.Series(rust_words).value_counts()\n",
    "go_freq = pd.Series(go_words).value_counts()\n",
    "\n",
    "all_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows word counts\n",
    "word_counts = (pd.concat([all_freq, js_freq, ts_freq, notspec_freq,solidity_freq, python_freq, rust_freq, go_freq], axis=1, sort=True)\n",
    "                .set_axis(['all', 'js', 'ts','notspec','solidity','python','rust','go'], axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of unique wordcounts\n",
    "js_freq.nunique(),ts_freq.nunique(),notspec_freq.nunique(),solidity_freq.nunique(),python_freq.nunique(),rust_freq.nunique(),go_freq.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67388bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most used words\n",
    "word_counts['all'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the percentage of spam vs ham\n",
    "(word_counts\n",
    " .assign(p_js=word_counts.js / word_counts['all'],\n",
    "         p_ts=word_counts.ts / word_counts['all'],\n",
    "         p_notspec=word_counts.notspec / word_counts['all'],\n",
    "         p_solidity=word_counts.solidity / word_counts['all'],\n",
    "         p_python=word_counts.python / word_counts['all'],\n",
    "         p_rust=word_counts.rust / word_counts['all'],\n",
    "         p_go=word_counts.go / word_counts['all'] \n",
    "        )\n",
    " .sort_values(by='all')\n",
    " [['p_js', 'p_ts','p_notspec','p_solidity','p_python','p_rust','p_rust','p_go']]\n",
    " .tail(25)\n",
    " .sort_values('p_js')\n",
    " .plot.barh(stacked=True))\n",
    "\n",
    "plt.title('Proportion of Language for the 25 most common words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 20 bigrams\n",
    "top_20_allwords_bigrams = (pd.Series(nltk.ngrams(all_words, 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_allwords_bigrams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4015a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams histogram\n",
    "top_20_allwords_bigrams.sort_values(ascending=False).plot.barh(color='pink', width=.9, figsize=(10, 6))\n",
    "\n",
    "plt.title('20 Most frequently occuring bigrams')\n",
    "plt.ylabel('Bigram')\n",
    "plt.xlabel('# Occurances')\n",
    "\n",
    "# make the labels pretty\n",
    "ticks, _ = plt.yticks()\n",
    "labels = top_20_allwords_bigrams.reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1])\n",
    "_ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams Word Cloud\n",
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_allwords_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f866e36",
   "metadata": {},
   "source": [
    "### Exploration Key Findings/Results\n",
    "Explore and visualize the natural language data that you have acquired. Here are some ideas for exploration:\n",
    "\n",
    "#### 1. The most 10 most common words and the number of appearances in the readme were:\n",
    "        \n",
    "        contract    3914\n",
    "        token       3460\n",
    "        run         1740\n",
    "        1           1722\n",
    "        smart       1622\n",
    "        ethereum    1530\n",
    "        project     1530\n",
    "        address     1503\n",
    "        detail      1499\n",
    "        install     1438\n",
    "        \n",
    "\n",
    "#### 2. Length of the readme did vary by programming language\n",
    "#### 3. Javascript and Typescript utilized numeric characters/words much more so than other programming languages.\n",
    "#### 4. Of the 1000 repositories that we pulled, the most common language breakdown is as follows:\n",
    "\n",
    "        JavaScript\t\t0.237624\n",
    "        TypeScript\t\t0.221782\n",
    "        Solidity\t\t0.165347\n",
    "        Not Specified\t0.127723\n",
    "        Python\t\t    0.058416\n",
    "        Rust\t\t    0.031683\n",
    "        Go\t\t        0.025743\n",
    "\n",
    "#### 5. The top ten bigrams that appeared and their number of occurences across all languages were:\n",
    "\n",
    "        (smart, contract)                           1348\n",
    "        (git, clone)                                 759\n",
    "        (detail, detailssummaryba)                   670\n",
    "        (styledisplayinline, width13)                414\n",
    "        (npm, install)                               350\n",
    "        (srchttpsgitioj9co9, styledisplayinline)     342\n",
    "        (codeimg, srchttpsgitioj9co9)                341\n",
    "        (npm, run)                                   321\n",
    "        (decimal, 18)                                315\n",
    "        (chainid, 1)                                 309\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e39a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb304cf",
   "metadata": {},
   "source": [
    "### Modeling - Classification\n",
    "baseline - 0.27\n",
    "1. decision tree\n",
    "2. random forest max depth 3\n",
    "3. random forest max depth 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b0219b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mlanguage\n\u001b[0;32m      7\u001b[0m X_train, X_test_val, y_train, y_test_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, stratify\u001b[38;5;241m=\u001b[39my, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.2\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m X_validate, X_test, y_validate, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2441\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2437\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[0;32m   2439\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m-> 2441\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2444\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2445\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2446\u001b[0m     )\n\u001b[0;32m   2447\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1600\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \n\u001b[0;32m   1572\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m-> 1600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1940\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1938\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1940\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1941\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1945\u001b[0m     )\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[0;32m   1951\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "# split data and feature selection \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df.readme_contents)\n",
    "y = df.language\n",
    "\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, stratify=y, test_size=.2)\n",
    "X_validate, X_test, y_validate, y_test = train_test_split(X_test_val, y_test_val, stratify=y_test_val, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn X_ into df with features \n",
    "X_train_df= pd.DataFrame(X_train.todense(), columns=tfidf.get_feature_names())\n",
    "X_validate_df= pd.DataFrame(X_validate.todense(), columns=tfidf.get_feature_names())\n",
    "X_test_df= pd.DataFrame(X_test.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn y_ into df\n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df for predictions\n",
    "predictions = pd.DataFrame({ \n",
    "    'actual': y_train.language\n",
    "})\n",
    "predictions['baseline'] = y_train[y_train['language'] == 'JavaScript'].shape[0] /y_train.shape[0]\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e3c02",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "\n",
    "max depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4dd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on train\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=123)\n",
    "clf = clf.fit(X_train_df, y_train)\n",
    "\n",
    "plt.figure(figsize=(13, 7))\n",
    "plot_tree(clf, feature_names=X_train_df.columns, class_names=clf.classes_, rounded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put train predicitions into df\n",
    "predictions['DT'] = clf.predict(X_train_df)\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_train_df, y_train)))\n",
    "predictions['DT'] = clf.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b1898",
   "metadata": {},
   "source": [
    "Random forest\n",
    "\n",
    "max depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=3, min_samples_leaf=3, random_state=123)\n",
    "rf.fit(X_train_df, y_train)\n",
    "y_pred = rf.predict(X_train_df)\n",
    "y_pred_proba = rf.predict_proba(X_train_df)\n",
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf.score(X_train_df, y_train)))\n",
    "\n",
    "predictions['RF'] = rf.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e382f0",
   "metadata": {},
   "source": [
    "Refined random forest\n",
    "\n",
    "max depth 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(max_depth=6, min_samples_leaf=3, random_state=123)\n",
    "rf2.fit(X_train_df, y_train)\n",
    "\n",
    "y_pred = rf2.predict(X_train_df)\n",
    "y_pred_proba = rf2.predict_proba(X_train_df)\n",
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf2.score(X_train_df, y_train)))\n",
    "\n",
    "predictions['RF2'] = rf2.score(X_train_df, y_train)\n",
    "\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56fbe21",
   "metadata": {},
   "source": [
    "# Testing on validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df for validate predictions\n",
    "predictions_val = pd.DataFrame({ \n",
    "    'actual': y_validate.language\n",
    "})\n",
    "# add baseline to predictions tables \n",
    "predictions_val['baseline'] = y_train[y_train['language'] == 'JavaScript'].shape[0] /y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c97cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decsion tree on validate\n",
    "\n",
    "predictions_val['DT_val'] = clf.predict(X_validate_df)\n",
    "\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "      .format(clf.score(X_validate_df, y_validate)))\n",
    "predictions_val['DT_val'] = clf.score(X_validate_df, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest on validate\n",
    "\n",
    "rf2 = RandomForestClassifier(max_depth=6, min_samples_leaf=3, random_state=123)\n",
    "rf2.fit(X_validate_df, y_validate)\n",
    "\n",
    "y_pred = rf2.predict(X_validate_df)\n",
    "y_pred_proba = rf2.predict_proba(X_validate_df)\n",
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf2.score(X_validate_df, y_validate)))\n",
    "\n",
    "predictions_val['RF2_val'] = rf2.score(X_validate_df, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30465728",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b1e54",
   "metadata": {},
   "source": [
    "All of the models beat the baseline on the training and validate data, with the best performing one being refined random forest with a max depth of 6. That model will be used on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c506bcf",
   "metadata": {},
   "source": [
    "## Test on random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df for test predictions\n",
    "predictions_test = pd.DataFrame({ \n",
    "    'actual': y_test.language\n",
    "})\n",
    "predictions_test['baseline'] = y_train[y_train['language'] == 'JavaScript'].shape[0] /y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a883d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest on test\n",
    "\n",
    "rf2 = RandomForestClassifier(max_depth=6, min_samples_leaf=3, random_state=123)\n",
    "rf2.fit(X_test_df, y_test)\n",
    "\n",
    "y_pred = rf2.predict(X_test_df)\n",
    "y_pred_proba = rf2.predict_proba(X_test_df)\n",
    "print('Accuracy of random forest classifier on training set: {:.2f}'\n",
    "     .format(rf2.score(X_test_df, y_test)))\n",
    "\n",
    "predictions_test['RF2_test'] = rf2.score(X_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e561d6",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "We stablish baseline, transform the data for modeling, fit the data on three different models, and build a function that will take in the text of a README file, and try to predict the programming language. We compared the performace of the models to each other and to the baseline. We determined refined random forest will be used on test data, and it beat the baseline.\n",
    "\n",
    "Next steps are potentially bringing in bigrams as additional features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e92b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
